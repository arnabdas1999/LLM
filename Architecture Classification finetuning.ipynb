{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a short story as text sample into Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The print command prints the total number of characters followed by the first 100\n",
    "characters of this file for illustration purposes. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20781\n",
      "THE VERDICT\n",
      "June 1908\n",
      "\n",
      "I had always thought Jack Gisburn rather a cheap genius--though a\n",
      "\n",
      "good fell\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Our goal is to tokenize this 20,479-character short story into individual words and special\n",
    "characters that we can then turn into embeddings for LLM training  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that it's common to process millions of articles and hundreds of thousands of\n",
    "books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
    "purposes, it's sufficient to work with smaller text samples like a single book to\n",
    "illustrate the main ideas behind the text processing steps and to make it possible to\n",
    "run it in reasonable time on consumer hardware. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
    "excursion and use Python's regular expression library re for illustration purposes. (Note\n",
    "that you don't have to learn or memorize any regular expression syntax since we will\n",
    "transition to a pre-built tokenizer later in this chapter.) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Using some simple example text, we can use the re.split command with the following\n",
    "syntax to split a text on whitespace characters:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The result is a list of individual words, whitespaces, and punctuation characters:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Let's modify the regular expression splits on whitespaces (\\s) and commas, and periods\n",
    "([,.]):</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We can see that the words and punctuation characters are now separate list entries just as\n",
    "we wanted\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
    "can remove these redundant characters safely as follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The tokenization scheme we devised above works well on the simple sample text. Let's\n",
    "modify it a bit further so that it can also handle other types of punctuation, such as\n",
    "question marks, quotation marks, and the double-dashes we have seen earlier in the first\n",
    "100 characters of Edith Wharton's short story, along with additional special characters: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now that we got a basic tokenizer working, let's apply it to Edith Wharton's entire short\n",
    "story:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE', 'VERDICT', 'June', '1908', 'I', 'had', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4667\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
    "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
    "them alphabetically to determine the vocabulary size:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "After determining that the vocabulary size is 1,130 via the above code, we create the\n",
    "vocabulary and print its first 51 entries for illustration purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "('1908', 8)\n",
      "(':', 9)\n",
      "(';', 10)\n",
      "('?', 11)\n",
      "('A', 12)\n",
      "('AM', 13)\n",
      "('Ah', 14)\n",
      "('Among', 15)\n",
      "('And', 16)\n",
      "('Are', 17)\n",
      "('Arrt', 18)\n",
      "('As', 19)\n",
      "('At', 20)\n",
      "('Be', 21)\n",
      "('Begin', 22)\n",
      "('Burlington', 23)\n",
      "('But', 24)\n",
      "('By', 25)\n",
      "('Carlo', 26)\n",
      "('Chicago', 27)\n",
      "('Claude', 28)\n",
      "('Come', 29)\n",
      "('Croft', 30)\n",
      "('Destroyed', 31)\n",
      "('Devonshire', 32)\n",
      "('Don', 33)\n",
      "('Dubarry', 34)\n",
      "('Emperors', 35)\n",
      "('End', 36)\n",
      "('FELT', 37)\n",
      "('Florence', 38)\n",
      "('For', 39)\n",
      "('Gallery', 40)\n",
      "('Gideon', 41)\n",
      "('Gisburn', 42)\n",
      "('Gisburns', 43)\n",
      "('Grafton', 44)\n",
      "('Greek', 45)\n",
      "('Grindle', 46)\n",
      "('Grindles', 47)\n",
      "('HAD', 48)\n",
      "('HAS', 49)\n",
      "('HAVE', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, based on the output above, the dictionary contains individual tokens\n",
    "associated with unique integer labels. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
    "text, we also need a way to turn token IDs into text. \n",
    "\n",
    "For this, we can create an inverse\n",
    "version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
    "passage from Edith Wharton's short story to try it out in practice:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 62, 2, 868, 1006, 619, 550, 764, 5, 1144, 614, 5, 1, 76, 7, 42, 869, 1126, 772, 812, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "Next, let's see if we can turn these token IDs back into text using the decode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the output above, we can see that the decode method successfully converted the\n",
    "token IDs back into the original text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
    "text based on a snippet from the training set. \n",
    "\n",
    "Let's now apply it to a new text sample that\n",
    "is not contained in the training set:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Hello, do you like tea?\"\n",
    "# print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now modify the vocabulary to include these two special tokens, <unk> and\n",
    "<|endoftext|>, by adding these to the list of all unique words that we created in the\n",
    "previous section:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the output of the print statement above, the new vocabulary size is 1132 (the\n",
    "vocabulary size in the previous section was 1130).\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As an additional quick check, let's print the last 5 entries of the updated vocabulary:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1145)\n",
      "('your', 1146)\n",
      "('yourself', 1147)\n",
      "('<|endoftext|>', 1148)\n",
      "('<|unk|>', 1149)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "A simple text tokenizer that handles unknown words</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Replace unknown words by <|unk|> tokens\n",
    "    \n",
    "Step 2: Replace spaces before the specified punctuations\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1149,\n",
       " 5,\n",
       " 372,\n",
       " 1144,\n",
       " 645,\n",
       " 993,\n",
       " 11,\n",
       " 1148,\n",
       " 61,\n",
       " 1006,\n",
       " 974,\n",
       " 1002,\n",
       " 740,\n",
       " 1006,\n",
       " 1149,\n",
       " 7]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on comparing the de-tokenized text above with the original input text, we know that\n",
    "the training dataset, Edith Wharton's short story The Verdict, did not contain the words\n",
    "\"Hello\" and \"palace.\"\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
    "above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
    "down words into subword units\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING (BPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We implemented a simple tokenization scheme in the previous sections for illustration\n",
    "purposes. \n",
    "\n",
    "This section covers a more sophisticated tokenization scheme based on a concept\n",
    "called byte pair encoding (BPE). \n",
    "\n",
    "The BPE tokenizer covered in this section was used to train\n",
    "LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Since implementing BPE can be relatively complicated, we will use an existing Python\n",
    "open-source library called tiktoken (https://github.com/openai/tiktoken). \n",
    "\n",
    "This library implements\n",
    "the BPE algorithm very efficiently based on source code in Rust.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\arnab\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\arnab\\anaconda3\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\arnab\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arnab\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arnab\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arnab\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arnab\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via\n",
    "an encode method:</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can then convert the token IDs back into text using the decode method, similar to our\n",
    "SimpleTokenizerV2 earlier:</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can make two noteworthy observations based on the token IDs and decoded text\n",
    "above. \n",
    "\n",
    "First, the <|endoftext|> token is assigned a relatively large token ID, namely,\n",
    "50256. \n",
    "\n",
    "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3,\n",
    "and the original model used in ChatGPT, has a total vocabulary size of 50,257, with\n",
    "<|endoftext|> being assigned the largest token ID.\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Second, the BPE tokenizer above encodes and decodes unknown words, such as\n",
    "\"someunknownPlace\" correctly. \n",
    "\n",
    "The BPE tokenizer can handle any unknown word. How does\n",
    "it achieve this without using <|unk|> tokens?\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary\n",
    "into smaller subword units or even individual characters.\n",
    "\n",
    "The enables it to handle out-ofvocabulary words. \n",
    "\n",
    "So, thanks to the BPE algorithm, if the tokenizer encounters an\n",
    "unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or\n",
    "characters\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us take another simple example to illustrate how the BPE tokenizer deals with unknown tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for GPT2 is: 50257\n",
      "The vocabulary size for GPT3 is: 50281\n",
      "The vocabulary size for GPT4 is: 100277\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
    "encodings = {\n",
    "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
    "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
    "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
    "}\n",
    "\n",
    "# Get the vocabulary size for each encoding\n",
    "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
    "\n",
    "# Print the vocabulary sizes\n",
    "for model, size in vocab_sizes.items():\n",
    "    print(f\"The vocabulary size for {model.upper()} is: {size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING INPUT-TARGET PAIRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with\n",
    "earlier using the BPE tokenizer introduced in the previous section:</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5774\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Executing the code above will return 5145, the total number of tokens in the training set,\n",
    "after applying the BPE tokenizer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
    "results in a slightly more interesting text passage in the next steps:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens\n",
    "and y contains the targets, which are the inputs shifted by 1:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The context size determines how many tokens are included in the input\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [11, 339, 550, 5710]\n",
      "y:      [339, 550, 5710, 465]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Processing the inputs along with the targets, which are the inputs shifted by one position,\n",
    "we can then create the next-word prediction tasks as\n",
    "follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] ----> 339\n",
      "[11, 339] ----> 550\n",
      "[11, 339, 550] ----> 5710\n",
      "[11, 339, 550, 5710] ----> 465\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token\n",
    "ID on the right side of the arrow represents the target token ID that the LLM is supposed to\n",
    "predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into\n",
    "text:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", ---->  he\n",
      ", he ---->  had\n",
      ", he had ---->  dropped\n",
      ", he had dropped ---->  his\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We've now created the input-target pairs that we can turn into use for the LLM training in\n",
    "upcoming chapters.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
    "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
    "can be thought of as multidimensional arrays.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In particular, we are interested in returning two tensors: an input tensor containing the\n",
    "text that the LLM sees and a target tensor that includes the targets for the LLM to predict,\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A DATA LOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and\n",
    "DataLoader classes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Tokenize the entire text\n",
    "    \n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return a single row from the dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
    "\n",
    "It defines how individual rows are fetched from the dataset. \n",
    "\n",
    "Each row consists of a number of\n",
    "token IDs (based on a max_length) assigned to an input_chunk tensor. \n",
    "\n",
    "The target_chunk\n",
    "tensor contains the corresponding targets. \n",
    "\n",
    "I recommend reading on to see how the data\n",
    "returned from this dataset looks like when we combine the dataset with a PyTorch\n",
    "DataLoader -- this will bring additional intuition and clarity.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch\n",
    "DataLoader:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "Step 2: Create dataset\n",
    "\n",
    "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes\n",
    "during training\n",
    "\n",
    "Step 4: The number of CPU processes to use for preprocessing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4, \n",
    "\n",
    "This will develop an intuition of how the GPTDatasetV1 class and the\n",
    "create_dataloader_v1 function work together: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "[tensor([[10970, 33310,    35, 18379]]), tensor([[33310,    35, 18379,   198]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs,\n",
    "and the second tensor stores the target token IDs. \n",
    "\n",
    "Since the max_length is set to 4, each of the two tensors contains 4 token IDs. \n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least\n",
    "256.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[33310,    35, 18379,   198]]), tensor([[   35, 18379,   198, 15749]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "If we compare the first with the second batch, we can see that the second batch's token\n",
    "IDs are shifted by one position compared to the first batch. \n",
    "\n",
    "For example, the second ID in\n",
    "the first batch's input is 367, which is the first ID of the second batch's input. \n",
    "\n",
    "The stride\n",
    "setting dictates the number of positions the inputs shift across batches, emulating a sliding\n",
    "window approach\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for\n",
    "illustration purposes. \n",
    "                                                                                 \n",
    "If you have previous experience with deep learning, you may know\n",
    "that small batch sizes require less memory during training but lead to more noisy model\n",
    "updates.\n",
    "\n",
    "Just like in regular deep learning, the batch size is a trade-off and hyperparameter\n",
    "to experiment with when training LLMs.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Before we move on to the two final sections of this chapter that are focused on creating\n",
    "the embedding vectors from the token IDs, let's have a brief look at how we can use the\n",
    "data loader to sample with a batch size greater than 1: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[10970, 33310,    35, 18379],\n",
      "        [  198, 15749, 40417,   198],\n",
      "        [  198,    40,   550,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  198,   198, 11274,  5891],\n",
      "        [ 1576,   438,   568,   340]])\n",
      "\n",
      "Targets:\n",
      " tensor([[33310,    35, 18379,   198],\n",
      "        [15749, 40417,   198,   198],\n",
      "        [   40,   550,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   198],\n",
      "        [  198, 11274,  5891,  1576],\n",
      "        [  438,   568,   340,   373]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a\n",
    "single word) but also avoid any overlap between the batches, since more overlap could lead\n",
    "to increased overfitting.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING TOKEN EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on\n",
    "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of\n",
    "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
    "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
    "setting the random seed to 123 for reproducibility purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The print statement in the code prints the embedding layer's underlying\n",
    "weight matrix:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "We can see that the weight matrix of the embedding layer contains small, random values.\n",
    "These values are optimized during LLM training as part of the LLM optimization itself, as we\n",
    "will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows\n",
    "and three columns. There is one row for each of the six possible tokens in the vocabulary.\n",
    "And there is one column for each of the three embedding dimensions.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the\n",
    "embedding vector:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we\n",
    "see that it is identical to the 4th row (Python starts with a zero index, so it's the row\n",
    "corresponding to index 3). In other words, the embedding layer is essentially a look-up\n",
    "operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Previously, we have seen how to convert a single token ID into a three-dimensional\n",
    "embedding vector. Let's now apply that to all four input IDs we defined earlier\n",
    "(torch.tensor([2, 3, 5, 1])):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Each row in this output matrix is obtained via a lookup operation from the embedding\n",
    "weight matrix\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Previously, we focused on very small embedding sizes in this chapter for illustration\n",
    "purposes. \n",
    "\n",
    "We now consider more realistic and useful embedding sizes and encode the input\n",
    "tokens into a 256-dimensional vector representation. \n",
    "\n",
    "This is smaller than what the original\n",
    "GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
    "for experimentation. \n",
    "\n",
    "Furthermore, we assume that the token IDs were created by the BPE\n",
    "tokenizer that we implemented earlier, which has a vocabulary size of 50,257:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Using the token_embedding_layer above, if we sample data from the data loader, we\n",
    "embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8\n",
    "with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's instantiate the data loader ( Data sampling with a sliding window),\n",
    "first:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[10970, 33310,    35, 18379],\n",
      "        [  198, 15749, 40417,   198],\n",
      "        [  198,    40,   550,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  198,   198, 11274,  5891],\n",
      "        [ 1576,   438,   568,   340]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch\n",
    "consists of 8 text samples with 4 tokens each.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now use the embedding layer to embed these token IDs into 256-dimensional\n",
    "vectors:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now\n",
    "embedded as a 256-dimensional vector.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For a GPT model's absolute embedding approach, we just need to create another\n",
    "embedding layer that has the same dimension as the token_embedding_layer:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
    "placeholder vector torch.arange(context_length), which contains a sequence of\n",
    "numbers 0, 1, ..., up to the maximum input length  1. \n",
    "\n",
    "The context_length is a variable\n",
    "that represents the supported input size of the LLM. \n",
    "\n",
    "Here, we choose it similar to the\n",
    "maximum length of the input text. \n",
    "\n",
    "In practice, input text can be longer than the supported\n",
    "context length, in which case we have to truncate the text.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors.\n",
    "We can now add these directly to the token embeddings, where PyTorch will add the 4x256-\n",
    "dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in\n",
    "each of the 8 batches:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The input_embeddings we created are the embedded input\n",
    "examples that can now be processed by the main LLM modules\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A SIMPLIFIED ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Consider the following input sentence, which has already been embedded into 3-\n",
    "dimensional vectors. \n",
    "\n",
    "We choose a small embedding dimension for\n",
    "illustration purposes to ensure it fits on the page without line breaks:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create 3D plot with vectors from origin to each point, using different colors\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Define a list of colors for the vectors\n",
    "# colors = ['r', 'g', 'b', 'c', 'm', 'y']\n",
    "\n",
    "# # Plot each vector with a different color and annotate with the corresponding word\n",
    "# for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "#     # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "#     ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "#     ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# # Set labels for axes\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "# # Set plot limits to keep arrows within the plot boundaries\n",
    "# ax.set_xlim([0, 1])\n",
    "# ax.set_ylim([0, 1])\n",
    "# ax.set_zlim([0, 1])\n",
    "\n",
    "# plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Each row represents a word, and each column represents an embedding dimension\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The second input token serves as the query    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the next step, we normalize each of the attention scores that\n",
    "we computed previously.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The main goal behind the normalization  is to obtain attention weights\n",
    "that sum up to 1. \n",
    "\n",
    "This normalization is a convention that is useful for interpretation and for\n",
    "maintaining training stability in an LLM. \n",
    "\n",
    "Here's a straightforward method for achieving this\n",
    "normalization step:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In practice, it's more common and advisable to use the softmax function for normalization.\n",
    "\n",
    "This approach is better at managing extreme values and offers more favorable gradient\n",
    "properties during training. \n",
    "\n",
    "Below is a basic implementation of the softmax function for\n",
    "normalizing the attention scores: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As the output shows, the softmax function also meets the objective and normalizes the\n",
    "attention weights such that they sum to 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In addition, the softmax function ensures that the attention weights are always positive.\n",
    "This makes the output interpretable as probabilities or relative importance, where higher\n",
    "weights indicate greater importance.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that this naive softmax implementation (softmax_naive) may encounter numerical\n",
    "instability problems, such as overflow and underflow, when dealing with large or small input\n",
    "values. \n",
    "\n",
    "Therefore, in practice, it's advisable to use the PyTorch implementation of softmax,\n",
    "which has been extensively optimized for performance:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In this case, we can see that it yields the same results as our previous softmax_naive\n",
    "function:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The context vector z(2)is calculated as a weighted sum of all input\n",
    "vectors. \n",
    "\n",
    "This involves multiplying each input vector by its corresponding attention weight:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# inputs2 = torch.tensor(\n",
    "#   [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "#    [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "#    [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "#    [0.22, 0.58, 0.33], # with     (x^4)\n",
    "#    [0.77, 0.25, 0.10], # one      (x^5)\n",
    "#    [0.05, 0.80, 0.55], # step     (x^6)\n",
    "#    [0.4419, 0.6515, 0.5683]]\n",
    "# )\n",
    "\n",
    "# # Corresponding words\n",
    "# words2 = ['Your', 'journey', 'starts', 'with', 'one', 'step', 'journey-context']\n",
    "\n",
    "# # Extract x, y, z coordinates\n",
    "# x_coords = inputs2[:, 0].numpy()\n",
    "# y_coords = inputs2[:, 1].numpy()\n",
    "# z_coords = inputs2[:, 2].numpy()\n",
    "\n",
    "# # Create 3D plot\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Plot each point and annotate with corresponding word\n",
    "# for x, y, z, word in zip(x_coords, y_coords, z_coords, words2):\n",
    "#     ax.scatter(x, y, z)\n",
    "#     ax.text(x, y, z, word, fontsize=10)\n",
    "\n",
    "# # Set labels for axes\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "# plt.title('3D Plot of Word Embeddings')\n",
    "# plt.show()\n",
    "\n",
    "# # Create 3D plot with vectors from origin to each point, using different colors\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# # Define a list of colors for the vectors\n",
    "# colors = ['r', 'g', 'b', 'c', 'm', 'y', 'r']\n",
    "\n",
    "# # Plot each vector with a different color and annotate with the corresponding word\n",
    "# for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words2, colors):\n",
    "#     # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "#     ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "#     ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# # Set labels for axes\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')\n",
    "# ax.set_zlabel('Z')\n",
    "\n",
    "# # Set plot limits to keep arrows within the plot boundaries\n",
    "# ax.set_xlim([0, 1])\n",
    "# ax.set_ylim([0, 1])\n",
    "# ax.set_zlim([0, 1])\n",
    "\n",
    "# plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, we can extend this computation to\n",
    "calculate attention weights and context vectors for all inputs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, we add an additional for-loop to compute the\n",
    "dot products for all pairs of inputs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Each element in the preceding tensor represents an attention score between each pair of\n",
    "inputs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "When computing the preceding attention score tensor, we used for-loops in Python.\n",
    "                                                            \n",
    "However, for-loops are generally slow, and we can achieve the same results using matrix\n",
    "multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We now normalize each row so that the values in\n",
    "each row sum to 1:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the context of using PyTorch, the dim parameter in functions like torch.softmax specifies\n",
    "the dimension of the input tensor along which the function will be computed. \n",
    "\n",
    "By setting\n",
    "dim=-1, we are instructing the softmax function to apply the normalization along the last\n",
    "dimension of the attn_scores tensor. \n",
    "\n",
    "If attn_scores is a 2D tensor (for example, with a\n",
    "shape of [rows, columns]), dim=-1 will normalize across the columns so that the values in\n",
    "each row (summing over the column dimension) sum up to 1.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's briefly verify that\n",
    "the rows indeed all sum to 1:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the third and last step, we now use these attention weights to compute all context\n",
    "vectors via matrix multiplication:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can double-check that the code is correct by comparing the 2nd row with the context\n",
    "vector z(2) calculated previously\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the result, we can see that the previously calculated context_vec_2 matches the\n",
    "second row in the previous tensor exactly\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "This concludes the code walkthrough of a simple self-attention mechanism.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's begin by defining a few variables:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "#A The second input element\n",
    "\n",
    "#B The input embedding size, d=3\n",
    "\n",
    "\n",
    "#C The output embedding size, d_out=2\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that in GPT-like models, the input and output dimensions are usually the same. \n",
    "\n",
    "But for illustration purposes, to better follow the computation, we choose different input (d_in=3)\n",
    "and output (d_out=2) dimensions here.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we initialize the three weight matrices Wq, Wk and Wv\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n"
     ]
    }
   ],
   "source": [
    "print(W_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "print(W_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that we are setting requires_grad=False to reduce clutter in the outputs for\n",
    "illustration purposes. \n",
    "\n",
    "If we were to use the weight matrices for model training, we\n",
    "would set requires_grad=True to update these matrices during model training.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we compute the query, key, and value vectors as shown earlier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see based on the output for the query, this results in a 2-dimensional vector. \n",
    "\n",
    "This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Even though our temporary goal is to only compute the one context vector z(2),  we still\n",
    "require the key and value vectors for all input elements. \n",
    "\n",
    "This is because they are involved in computing the attention weights with respect to the query q(2)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can obtain all keys and values via matrix multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "queries.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "print(\"queries.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can tell from the outputs, we successfully projected the 6 input tokens from a 3D\n",
    "onto a 2D embedding space:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, let's compute the attention score 22</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Again, we can generalize this computation to all attention scores via matrix multiplication:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T # omega\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We compute the attention weights by scaling the\n",
    "attention scores and using the softmax function we used earlier. \n",
    "\n",
    "The difference to earlier is\n",
    "that we now scale the attention scores by dividing them by the square root of the\n",
    "embedding dimension of the keys. \n",
    "\n",
    "Note that taking the square root is mathematically the\n",
    "same as exponentiating by 0.5:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHY DIVIDE BY SQRT (DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Reason 1: For stability in learning\n",
    "\n",
    "The softmax function is sensitive to the magnitudes of its inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky,\" where the highest value receives almost all the probability mass, and the rest receive very little.\n",
    "\n",
    "In attention mechanisms, particularly in transformers, if the dot products between query and key vectors become too large (like multiplying by 8 in this example), the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "Softmax after scaling (tensor * 8): tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the tensor\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "# Apply softmax without scaling\n",
    "softmax_result = torch.softmax(tensor, dim=-1)\n",
    "print(\"Softmax without scaling:\", softmax_result)\n",
    "\n",
    "# Multiply the tensor by 8 and then apply softmax\n",
    "scaled_tensor = tensor * 8\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
    "print(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUT WHY SQRT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Reason 2: To make the variance of the dot product stable\n",
    "\n",
    "The dot product of  Q and K increases the variance because multiplying two random numbers increases the variance.\n",
    "\n",
    "The increase in variance grows with the dimension. \n",
    "\n",
    "Dividing by sqrt (dimension) keeps the variance close to 1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 4.6011872301089465\n",
      "Variance after scaling (dim=5): 0.9202374460217893\n",
      "Variance before scaling (dim=100): 99.60980332012049\n",
      "Variance after scaling (dim=100): 0.996098033201205\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trials=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "        \n",
    "        # Compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "        \n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "    \n",
    "    # Calculate variance of the dot products\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "# For dimension 20\n",
    "variance_before_100, variance_after_100 = compute_variance(100)\n",
    "print(f\"Variance before scaling (dim=100): {variance_before_100}\")\n",
    "print(f\"Variance after scaling (dim=100): {variance_after_100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We now compute the context vector as a weighted sum over the value\n",
    "vectors. \n",
    "\n",
    "Here, the attention weights serve as a weighting factor that weighs the respective\n",
    "importance of each value vector. \n",
    "\n",
    "We can use matrix multiplication to\n",
    "obtain the output in one step:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "So far, we only computed a single context vector, z(2). \n",
    "\n",
    "In the next section, we will generalize the code to compute all context vectors in the input sequence, z(1)to z (T)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "In the previous sections, we have gone through a lot of steps to compute the self-attention\n",
    "outputs. \n",
    "\n",
    "This was mainly done for illustration purposes so we could go through one step at\n",
    "a time. \n",
    "\n",
    "In practice, with the LLM implementation in the next chapter in mind, it is helpful to\n",
    "organize this code into a Python class as follows:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a\n",
    "fundamental building block of PyTorch models, which provides necessary functionalities for\n",
    "model layer creation and management.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The __init__ method initializes trainable weight matrices (W_query, W_key, and\n",
    "W_value) for queries, keys, and values, each transforming the input dimension d_in to an\n",
    "output dimension d_out.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "During the forward pass, using the forward method, we compute the attention scores\n",
    "(attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Finally, we create a context vector by weighting the values with these normalized attention\n",
    "scores.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Since inputs contains six embedding vectors, we get a matrix storing the six\n",
    "context vectors, as shown in the above result. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As a quick check, notice how the second row ([0.3061, 0.8210]) matches the contents of\n",
    "context_vec_2 in the previous section.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's\n",
    "nn.Linear layers, which effectively perform matrix multiplication when the bias units are\n",
    "disabled. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Additionally, a significant advantage of using nn.Linear instead of manually\n",
    "implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
    "initialization scheme, contributing to more stable and effective model training.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they\n",
    "use different initial weights for the weight matrices since nn.Linear uses a more\n",
    "sophisticated weight initialization scheme.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's work with the attention scores and weights from the previous section to code the causal attention mechanism.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the first step illustrated in Figure 3.20, we compute the attention weights using the\n",
    "softmax function as we have done in previous sections:    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Reuse the query and key weight matrices of the SelfAttention_v2 object from the previous section for\n",
    "convenience\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs) #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can now use PyTorch's tril function to create a mask\n",
    "where the values above the diagonal are zero:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(context_length, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, we can multiply this mask with the attention weights to zero out the values above the\n",
    "diagonal:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the elements above the diagonal are successfully zeroed out\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The third step is to renormalize the attention weights to sum up to 1 again in\n",
    "each row. \n",
    "\n",
    "We can achieve this by dividing each element in each row by the sum in each\n",
    "row:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The result is an attention weight matrix where the attention weights above the diagonal are\n",
    "zeroed out and where the rows sum to 1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "While we could be technically done with implementing causal attention at this point, we can\n",
    "take advantage of a mathematical property of the softmax function. \n",
    "\n",
    "We can implement the computation of the masked attention weights more efficiently in fewer steps.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The softmax function converts its inputs into a probability distribution. \n",
    "\n",
    "When negative\n",
    "infinity values (-) are present in a row, the softmax function treats them as zero\n",
    "probability. \n",
    "\n",
    "(Mathematically, this is because e\n",
    "- approaches 0.)\n",
    "\n",
    "\n",
    "We can implement this more efficient masking \"trick\" by creating a mask with 1's above\n",
    "the diagonal and then replacing these 1's with negative infinity (-inf) values:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
      "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
      "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
      "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
      "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
      "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, all we need to do is apply the softmax function to these masked results, and we are\n",
    "done.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the output, the values in each row sum to 1, and no further\n",
    "normalization is necessary.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Masking in Transformers sets scores for future tokens to a large negative value, making their influence in the softmax calculation effectively zero. \n",
    "\n",
    "The softmax function then recalculates attention weights only among the unmasked tokens. \n",
    "\n",
    "This process ensures no information leakage from masked tokens, focusing the model solely on the intended data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We could now use the modified attention weights to compute the context vectors via\n",
    "context_vec = attn_weights @ values.\n",
    "\n",
    "However, in the next section,\n",
    "we first cover another minor tweak to the causal attention mechanism that is useful for\n",
    "reducing overfitting when training LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASKING ADDITIONAL ATTENTION WEIGHTS WITH DROPOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the following code example, we use a dropout rate of 50%, which means masking out\n",
    "half of the attention weights.\n",
    "\n",
    "When we train the GPT model in later chapters, we will use a\n",
    "lower dropout rate, such as 0.1 or 0.2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the following code, we apply PyTorch's dropout implementation first to a 66 tensor\n",
    "consisting of ones for illustration purposes:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(6, 6) #B\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
    "elements in the matrix are randomly set to zero. \n",
    "\n",
    "To compensate for the reduction in active\n",
    "elements, the values of the remaining elements in the matrix are scaled up by a factor of\n",
    "1/0.5 =2. \n",
    "\n",
    "This scaling is crucial to maintain the overall balance of the attention weights,\n",
    "ensuring that the average influence of the attention mechanism remains consistent during\n",
    "both the training and inference phases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, let's apply dropout to the attention weight matrix itself:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see above, the resulting attention weight matrix now has additional elements zeroed out and the\n",
    "remaining ones rescaled.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Having gained an understanding of causal attention and dropout masking, we will\n",
    "develop a concise Python class in the following section. \n",
    "\n",
    "This class is designed to facilitate\n",
    "the efficient application of these two techniques.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we will now incorporate the causal attention and dropout modifications into\n",
    "the SelfAttention Python class we developed in section 3.4. \n",
    "\n",
    "This class will then serve as a\n",
    "template for developing multi-head attention in the upcoming section.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we begin, one more thing is to ensure that the code can handle batches\n",
    "consisting of more than one input. \n",
    "\n",
    "This will ensure that the CausalAttention class supports the batch\n",
    "outputs produced by the data loader we implemented earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    " 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token\n",
    "is a 3-dimensional embedding vector.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The following CausalAttention class is similar to the SelfAttention class we\n",
    "implemented earlier, except that we now added the dropout and causal mask components\n",
    "as highlighted in the following code.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Compared to the previous SelfAttention_v1 class, we added a dropout layer.\n",
    "    \n",
    "Step 2: The register_buffer call is also a new addition (more information is provided in the following text).\n",
    "\n",
    "Step 3:  We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).\n",
    "\n",
    "Step 4: In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory\n",
    "copies\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The use of register_buffer in\n",
    "PyTorch is not strictly necessary for all use cases but offers several advantages here. \n",
    "\n",
    "For\n",
    "instance, when we use the CausalAttention class in our LLM, buffers are automatically\n",
    "moved to the appropriate device (CPU or GPU) along with our model, which will be relevant\n",
    "when training the LLM in future chapters. \n",
    "\n",
    "This means we don't need to manually ensure\n",
    "these tensors are on the same device as your model parameters, avoiding device mismatch\n",
    "errors.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can use the CausalAttention class as follows, similar to SelfAttention previously:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the resulting context vector is a 3D tensor where each token is now represented by a 2D\n",
    "embedding:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the next section, we will expand on this concept\n",
    "and implement a multi-head attention module, that implements several of such causal\n",
    "attention mechanisms in parallel.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTENDING SINGLE HEAD ATTENTION TO MULTI-HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In practical terms, implementing multi-head attention involves creating multiple instances\n",
    "of the self-attention mechanism, each with\n",
    "its own weights, and then combining their outputs\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper\n",
    "class that stacks multiple instances of our previously implemented CausalAttention\n",
    "module:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via\n",
    "num_heads=2) and CausalAttention output dimension d_out=2, this results in a 4-\n",
    "dimensional context vectors (d_out*num_heads=4)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To illustrate further with a concrete example, we can use the\n",
    "MultiHeadAttentionWrapper class similar to the CausalAttention class before:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens = 6\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The first dimension of the resulting context_vecs tensor is 2 since we have two input texts\n",
    "(the input texts are duplicated, which is why the context vectors are exactly the same for\n",
    "those). \n",
    "\n",
    "The second dimension refers to the 6 tokens in each input. The third dimension\n",
    "refers to the 4-dimensional embedding of each token.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section, we implemented a MultiHeadAttentionWrapper that combined multiple\n",
    "single-head attention modules. \n",
    "\n",
    "However, note that these are processed sequentially via\n",
    "[head(x) for head in self.heads] in the forward method. \n",
    "\n",
    "We can improve this\n",
    "implementation by processing the heads in parallel. \n",
    "\n",
    "One way to achieve this is by\n",
    "computing the outputs for all attention heads simultaneously via matrix multiplication, as\n",
    "we will explore in the next section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and\n",
    "CausalAttention, we can combine both of these concepts into a single\n",
    "MultiHeadAttention class. \n",
    "\n",
    "Also, in addition to just merging the\n",
    "MultiHeadAttentionWrapper with the CausalAttention code, we will make some other\n",
    "modifications to implement multi-head attention more efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list\n",
    "of CausalAttention objects (self.heads), each representing a separate attention head.\n",
    "\n",
    "\n",
    "The CausalAttention class independently performs the attention mechanism, and the\n",
    "results from each head are concatenated.\n",
    "\n",
    "In contrast, the following MultiHeadAttention\n",
    "class integrates the multi-head functionality within a single class. \n",
    "\n",
    "\n",
    "It splits the input into\n",
    "multiple heads by reshaping the projected query, key, and value tensors and then combines\n",
    "the results from these heads after computing attention.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's take a look at the MultiHeadAttention class before we discuss it further:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Reduce the projection dim to match desired output dim\n",
    "\n",
    "Step 2: Use a Linear layer to combine head outputs\n",
    "\n",
    "Step 3: Tensor shape: (b, num_tokens, d_out)\n",
    "\n",
    "Step 4: We implicitly split the matrix by adding a `num_heads` dimension. Then we unroll last dim: (b,\n",
    "num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "Step 5: Transpose from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "Step 6: Compute dot product for each head\n",
    "\n",
    "Step 7: Mask truncated to the number of tokens\n",
    "\n",
    "Step 8: Use the mask to fill attention scores\n",
    "\n",
    "Step 9: Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "\n",
    "Step 10: Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "\n",
    "Step 11: Add an optional linear projection\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Even though the reshaping (.view) and transposing (.transpose) of tensors inside the\n",
    "MultiHeadAttention class looks very complicated, mathematically, the\n",
    "MultiHeadAttention class implements the same concept as the\n",
    "MultiHeadAttentionWrapper earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked\n",
    "multiple single-head attention layers that we combined into a multi-head attention layer.\n",
    "\n",
    "\n",
    "The MultiHeadAttention class takes an integrated approach. \n",
    "\n",
    "It starts with a multi-head\n",
    "layer and then internally splits this layer into individual attention heads\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DETAILED EXPLANATION OF THE MULTI-HEAD ATTENTION CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The splitting of the query, key, and value tensors, is achieved\n",
    "through tensor reshaping and transposing operations using PyTorch's .view and\n",
    ".transpose methods. \n",
    "\n",
    "The input is first transformed (via linear layers for queries, keys, and\n",
    "values) and then reshaped to represent multiple heads.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The key operation is to split the d_out dimension into num_heads and head_dim, where\n",
    "head_dim = d_out / num_heads. \n",
    "\n",
    "This splitting is then achieved using the .view method: a\n",
    "tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokens,\n",
    "num_heads, head_dim).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The tensors are then transposed to bring the num_heads dimension before the\n",
    "num_tokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim).\n",
    "\n",
    "This transposition is crucial for correctly aligning the queries, keys, and values across the\n",
    "different heads and performing batched matrix multiplications efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To illustrate this batched matrix multiplication, suppose we have the following example\n",
    "tensor:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Continuing with MultiHeadAttention, after computing the attention weights and context\n",
    "vectors, the context vectors from all heads are transposed back to the shape (b,\n",
    "num_tokens, num_heads, head_dim). \n",
    "\n",
    "These vectors are then reshaped (flattened) into the\n",
    "shape (b, num_tokens, d_out), effectively combining the outputs from all heads\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Additionally, we added a so-called output projection layer (self.out_proj) to\n",
    "MultiHeadAttention after combining the heads, which is not present in the\n",
    "CausalAttention class. \n",
    "\n",
    "This output projection layer is not strictly necessary, but it is commonly used in many LLM\n",
    "architectures, which is why we added it here for completeness.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Even though the MultiHeadAttention class looks more complicated than the\n",
    "MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors,\n",
    "it is more efficient. \n",
    "\n",
    "The reason is that we only need one matrix multiplication to compute\n",
    "the keys, for instance, keys = self.W_key(x) (the same is true for the queries and\n",
    "values). \n",
    "                                              \n",
    "\n",
    "In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication,\n",
    "which is computationally one of the most expensive steps, for each attention head.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The MultiHeadAttention class can be used similar to the SelfAttention and\n",
    "CausalAttention classes we implemented earlier:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the results, the output dimension is directly controlled by the\n",
    "d_out argument:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In this section, we implemented the MultiHeadAttention class that we will use in the\n",
    "upcoming sections when implementing and training the LLM itself. \n",
    "\n",
    "\n",
    "Note that while the code is fully functional, we used relatively small embedding sizes and numbers of attention\n",
    "heads to keep the outputs readable.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "For comparison, the smallest GPT-2 model (117 million parameters) has 12 attention\n",
    "heads and a context vector embedding size of 768. \n",
    "\n",
    "The largest GPT-2 model (1.5 billion\n",
    "parameters) has 25 attention heads and a context vector embedding size of 1600.\n",
    "\n",
    "Note\n",
    "that the embedding sizes of the token inputs and context embeddings are the same in GPT\n",
    "models (d_in = d_out).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 1: DUMMY GPT MODEL CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Use a placeholder for TransformerBlock\n",
    "\n",
    "Step 2: Use a placeholder for LayerNorm\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The DummyGPTModel class in this code defines a simplified version of a GPT-like model using\n",
    "PyTorch's neural network module (nn.Module). \n",
    "\n",
    "The model architecture in the\n",
    "DummyGPTModel class consists of token and positional embeddings, dropout, a series of\n",
    "transformer blocks (DummyTransformerBlock), a final layer normalization\n",
    "(DummyLayerNorm), and a linear output layer (out_head). \n",
    "\n",
    "The configuration is passed in via\n",
    "a Python dictionary, for instance, the GPT_CONFIG_124M dictionary we created earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The forward method describes the data flow through the model: it computes token and\n",
    "positional embeddings for the input indices, applies dropout, processes the data through\n",
    "the transformer blocks, applies normalization, and finally produces logits with the linear\n",
    "output layer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The code above is already functional, as we will see later in this section after we prepare\n",
    "the input data. \n",
    "\n",
    "However, for now, note in the code above that we have used placeholders\n",
    "(DummyLayerNorm and DummyTransformerBlock) for the transformer block and layer\n",
    "normalization, which we will develop in later sections\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we will prepare the input data and initialize a new GPT model to illustrate its\n",
    "usage.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: CREATE AN INSTANCE OF DUMMYGPTMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The output tensor has two rows corresponding to the two text samples. Each text sample\n",
    "consists of 4 tokens; each token is a 50,257-dimensional vector, which matches the size of\n",
    "the tokenizer's vocabulary.\n",
    "\n",
    "\n",
    "The embedding has 50,257 dimensions because each of these dimensions refers to a\n",
    "unique token in the vocabulary. At the end of this chapter, when we implement the\n",
    "postprocessing code, we will convert these 50,257-dimensional vectors back into token IDs,\n",
    "which we can then decode into words.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Now that we have taken a top-down look at the GPT architecture and its in- and outputs,\n",
    "we will code the individual placeholders in the upcoming sections, starting with the real\n",
    "layer normalization class that will replace the DummyLayerNorm in the previous code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 2: LAYER NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation with a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The neural network layer we have coded consists of a Linear layer followed by a non-linear\n",
    "activation function, ReLU (short for Rectified Linear Unit), which is a standard activation\n",
    "function in neural networks. \n",
    "\n",
    "If you are unfamiliar with ReLU, it simply thresholds negative\n",
    "inputs to 0, ensuring that a layer outputs only positive values, which explains why the\n",
    "resulting layer output does not contain any negative values. \n",
    "\n",
    "(Note that we will use another,\n",
    "more sophisticated activation function in GPT, which we will introduce in the next section).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we apply layer normalization to these outputs, let's examine the mean and\n",
    "variance:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The first row in the mean tensor above contains the mean value for the first input row, and\n",
    "the second output row contains the mean for the second input row.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Using keepdim=True in operations like mean or variance calculation ensures that the\n",
    "output tensor retains the same number of dimensions as the input tensor, even though the\n",
    "operation reduces the tensor along the dimension specified via dim. \n",
    "\n",
    "For instance, without\n",
    "keepdim=True, the returned mean tensor would be a 2-dimensional vector [0.1324,\n",
    "0.2170] instead of a 21-dimensional matrix [[0.1324], [0.2170]].\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "For a 2D tensor (like a matrix), using dim=-1 for operations such as\n",
    "mean or variance calculation is the same as using dim=1. \n",
    "\n",
    "This is because -1 refers to the\n",
    "tensor's last dimension, which corresponds to the columns in a 2D tensor. \n",
    "\n",
    "Later, when\n",
    "adding layer normalization to the GPT model, which produces 3D tensors with shape\n",
    "[batch_size, num_tokens, embedding_size], we can still use dim=-1 for normalization\n",
    "across the last dimension, avoiding a change from dim=1 to dim=2.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, let us apply layer normalization to the layer outputs we obtained earlier. The\n",
    "operation consists of subtracting the mean and dividing by the square root of the variance\n",
    "(also known as standard deviation):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that the value 2.9802e-08 in the output tensor is the scientific notation for 2.9802 \n",
    "10-8, which is 0.0000000298 in decimal form. This value is very close to 0, but it is not\n",
    "exactly 0 due to small numerical errors that can accumulate because of the finite precision\n",
    "with which computers represent numbers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To improve readability, we can also turn off the scientific notation when printing tensor\n",
    "values by setting sci_mode to False:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now encapsulate this process in a PyTorch module that we can use in the GPT\n",
    "model later:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "This specific implementation of layer Normalization operates on the last dimension of the\n",
    "input tensor x, which represents the embedding dimension (emb_dim). \n",
    "\n",
    "The variable eps is a\n",
    "small constant (epsilon) added to the variance to prevent division by zero during\n",
    "normalization. \n",
    "\n",
    "The scale and shift are two trainable parameters (of the same dimension\n",
    "as the input) that the LLM automatically adjusts during training if it is determined that\n",
    "doing so would improve the model's performance on its training task. \n",
    "\n",
    "This allows the model\n",
    "to learn appropriate scaling and shifting that best suit the data it is processing.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_A small note on biased variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In our variance calculation method, we have opted for an implementation detail by\n",
    "setting unbiased=False. \n",
    "\n",
    "For those curious about what this means, in the variance\n",
    "calculation, we divide by the number of inputs n in the variance formula. \n",
    "\n",
    "This approach does not apply Bessel's correction, which typically uses n-1 instead of n in\n",
    "the denominator to adjust for bias in sample variance estimation. \n",
    "\n",
    "This decision results in a so-called biased estimate of the variance. \n",
    "\n",
    "For large-scale language\n",
    "models (LLMs), where the embedding dimension n is significantly large, the\n",
    "difference between using n and n-1 is practically negligible. \n",
    "\n",
    "We chose this approach to ensure compatibility with the GPT-2 model's normalization layers and because it\n",
    "reflects TensorFlow's default behavior, which was used to implement the original GPT2 model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now try the LayerNorm module in practice and apply it to the batch input:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n"
     ]
    }
   ],
   "source": [
    "print(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the results, the layer normalization code works as expected and\n",
    "normalizes the values of each of the two inputs such that they have a mean of 0 and a\n",
    "variance of 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 3: FEEDFORWARD NEURAL NETWORK WITH GELU ACTIVATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement the GELU activation function approximation used by GPT-2:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To get an idea of what this GELU function looks like and how it compares to the ReLU\n",
    "function, let's plot these functions side by side:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see in the resulting plot, ReLU is a piecewise linear function that\n",
    "outputs the input directly if it is positive; otherwise, it outputs zero. \n",
    "\n",
    "GELU is a smooth, nonlinear function that approximates ReLU but with a non-zero gradient for negative values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The smoothness of GELU, as shown in the above figure, can lead to better optimization properties\n",
    "during training, as it allows for more nuanced adjustments to the model's parameters. \n",
    "\n",
    "In contrast, ReLU has a sharp corner at zero, which can sometimes make optimization harder,\n",
    "especially in networks that are very deep or have complex architectures. \n",
    "\n",
    "Moreover, unlike RELU, which outputs zero for any negative input, GELU allows for a small, non-zero output\n",
    "for negative values. \n",
    "\n",
    "This characteristic means that during the training process, neurons that\n",
    "receive negative input can still contribute to the learning process, albeit to a lesser extent\n",
    "than positive inputs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, let's use the GELU function to implement the small neural network module,\n",
    "FeedForward, that we will be using in the LLM's transformer block later:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see in the preceding code, the FeedForward module is a small neural network\n",
    "consisting of two Linear layers and a GELU activation function. \n",
    "\n",
    "In the 124 million parameter GPT model, it receives the input batches with tokens that have an embedding\n",
    "size of 768 each via the GPT_CONFIG_124M dictionary where GPT_CONFIG_124M[\"emb_dim\"]\n",
    "= 768.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's use the GELU function to implement the small neural network module,\n",
    "FeedForward, that we will be using in the LLM's transformer block later:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The FeedForward module we implemented in this section plays a crucial role in enhancing\n",
    "the model's ability to learn from and generalize the data. \n",
    "\n",
    "\n",
    "Although the input and output dimensions of this module are the same, it internally expands the embedding dimension\n",
    "into a higher-dimensional space through the first linear layer.\n",
    "\n",
    "This expansion is followed by a non-linear GELU activation, and then a contraction back to\n",
    "the original dimension with the second linear transformation. \n",
    "\n",
    "Such a design allows for the\n",
    "exploration of a richer representation space.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Moreover, the uniformity in input and output dimensions simplifies the architecture by\n",
    "enabling the stacking of multiple layers, as we will do later, without the need to adjust\n",
    "dimensions between them, thus making the model more scalable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 4: SHORTCUT CONNECTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let us see how we can add shortcut connections to the forward method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The code implements a deep neural network with 5 layers, each consisting of a Linear\n",
    "layer and a GELU activation function. \n",
    "\n",
    "In the forward pass, we iteratively pass the input\n",
    "through the layers and optionally add the shortcut connections  if\n",
    "the self.use_shortcut attribute is set to True.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's use this code to first initialize a neural network without shortcut connections. Here,\n",
    "each layer will be initialized such that it accepts an example with 3 input values and returns\n",
    "3 output values. The last layer returns a single output value:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we implement a function that computes the gradients in the the model's backward\n",
    "pass:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the preceding code, we specify a loss function that computes how close the model output\n",
    "and a user-specified target (here, for simplicity, the value 0) are. \n",
    "\n",
    "Then, when calling loss.backward(), PyTorch computes the loss gradient for each layer in the model. \n",
    "\n",
    "We can iterate through the weight parameters via model.named_parameters(). \n",
    "\n",
    "Suppose we have a 33 weight parameter matrix for a given layer. \n",
    "\n",
    "In that case, this layer will have 33 gradient values, and we print the mean absolute gradient of these 33 gradient values to\n",
    "obtain a single gradient value per layer to compare the gradients between layers more\n",
    "easily.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In short, the .backward() method is a convenient method in PyTorch that computes loss\n",
    "gradients, which are required during model training, without implementing the math for the\n",
    "gradient calculation ourselves, thereby making working with deep neural networks much\n",
    "more accessible. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now use the print_gradients function and apply it to the model without skip\n",
    "connections:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see based on the output of the print_gradients function, the gradients become\n",
    "smaller as we progress from the last layer (layers.4) to the first layer (layers.0), which\n",
    "is a phenomenon called the vanishing gradient problem.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now instantiate a model with skip connections and see how it compares:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, based on the output, the last layer (layers.4) still has a larger gradient\n",
    "than the other layers. \n",
    "\n",
    "However, the gradient value stabilizes as we progress towards the\n",
    "first layer (layers.0) and doesn't shrink to a vanishingly small value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In conclusion, shortcut connections are important for overcoming the limitations posed\n",
    "by the vanishing gradient problem in deep neural networks. \n",
    "\n",
    "Shortcut connections are a core building block of very large models such as LLMs, and they will help facilitate more effective\n",
    "training by ensuring consistent gradient flow across layers when we train the GPT model \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 5: CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE BUILDING BLOCKS: LAYER NORMALIZATION, GELU AND FEED-FORWARD NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let us code a transformer block as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Shortcut connection for attention block\n",
    "\n",
    "Step 2:  Shortcut connection for feed forward block\n",
    "\n",
    "Step 3: Add the original input back\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The given code defines a TransformerBlock class in PyTorch that includes a multi-head\n",
    "attention mechanism (MultiHeadAttention) and a feed forward network (FeedForward),\n",
    "both configured based on a provided configuration dictionary (cfg), such as\n",
    "GPT_CONFIG_124M\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Layer normalization (LayerNorm) is applied before each of these two components, and\n",
    "dropout is applied after them to regularize the model and prevent overfitting. \n",
    "\n",
    "This is also known as Pre-LayerNorm. \n",
    "\n",
    "Older architectures, such as the original transformer model,\n",
    "applied layer normalization after the self-attention and feed-forward networks instead,\n",
    "known as Post-LayerNorm, which often leads to worse training dynamics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The class also implements the forward pass, where each component is followed by a\n",
    "shortcut connection that adds the input of the block to its output. This critical feature helps\n",
    "gradients flow through the network during training and improves the learning of deep\n",
    "models \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the GPT_CONFIG_124M dictionary we defined earlier, let's instantiate a transformer\n",
    "block and feed it some sample data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Create sample input of shape [batch_size, num_tokens, emb_dim]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see from the code output, the transformer block maintains the input dimensions\n",
    "in its output, indicating that the transformer architecture processes sequences of data\n",
    "without altering their shape throughout the network.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The preservation of shape throughout the transformer block architecture is not incidental\n",
    "but a crucial aspect of its design. \n",
    "\n",
    "This design enables its effective application across a wide\n",
    "range of sequence-to-sequence tasks, where each output vector directly corresponds to an\n",
    "input vector, maintaining a one-to-one relationship. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "However, the output is a context vector\n",
    "that encapsulates information from the entire input sequence.\n",
    "\n",
    "This means that while the physical dimensions of the sequence (length and feature size)\n",
    "remain unchanged as it passes through the transformer block, the content of each output\n",
    "vector is re-encoded to integrate contextual information from across the entire input\n",
    "sequence.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 6: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We started with this: A dummy GPT model class\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Then, we coded the Layer Normalization class, Feedforward Neural Network class and also the Transformer class\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "LAYER NORMALIZATION AND FEEDFORWARD NEURAL NETWORK CLASS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "TRANSFORMER CLASS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, let us use all this knowledge and code the entire GPT architecture\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The __init__ constructor of this GPTModel class initializes the token and positional\n",
    "embedding layers using the configurations passed in via a Python dictionary, cfg. \n",
    "\n",
    "These\n",
    "embedding layers are responsible for converting input token indices into dense vectors and\n",
    "adding positional information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Next, the __init__ method creates a sequential stack of TransformerBlock modules\n",
    "equal to the number of layers specified in cfg. \n",
    "\n",
    "Following the transformer blocks, a\n",
    "LayerNorm layer is applied, standardizing the outputs from the transformer blocks to\n",
    "stabilize the learning process. \n",
    "\n",
    "Finally, a linear output head without bias is defined, which\n",
    "projects the transformer's output into the vocabulary space of the tokenizer to generate\n",
    "logits for each token in the vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The forward method takes a batch of input token indices, computes their embeddings,\n",
    "applies the positional embeddings, passes the sequence through the transformer blocks,\n",
    "normalizes the final output, and then computes the logits, representing the next token's\n",
    "unnormalized probabilities. We will convert these logits into tokens and text outputs in the\n",
    "next section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now initialize the 124 million parameter GPT model using the GPT_CONFIG_124M\n",
    "dictionary we pass into the cfg parameter and feed it with the batch text input we created\n",
    "at the beginning of this chapter:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the output tensor has the shape [2, 4, 50257], since we passed in 2 input\n",
    "texts with 4 tokens each. The last dimension, 50,257, corresponds to the vocabulary size of\n",
    "the tokenizer. In the next section, we will see how to convert each of these 50,257-\n",
    "dimensional output vectors back into tokens.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the numel() method, short for \"number of elements,\" we can collect the total\n",
    "number of parameters in the model's parameter tensors:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Earlier, we spoke of initializing a 124\n",
    "million parameter GPT model, so why is the actual number of parameters 163 million, as\n",
    "shown in the preceding code output?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "The reason is a concept called weight tying that is used in the original GPT-2\n",
    "architecture, which means that the original GPT-2 architecture is reusing the weights from\n",
    "the token embedding layer in its output layer. \n",
    "\n",
    "To understand what this means, let's take a\n",
    "look at the shapes of the token embedding layer and linear output layer that we initialized\n",
    "on the model via the GPTModel earlier:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the print outputs, the weight tensors for both these layers have the\n",
    "same shape:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The token embedding and output layers are very large due to the number of rows for the\n",
    "50,257 in the tokenizer's vocabulary. Let's remove the output layer parameter count from\n",
    "the total GPT-2 model count according to the weight tying:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the model is now only 124 million parameters large, matching the original\n",
    "size of the GPT-2 model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Weight tying reduces the overall memory footprint and computational complexity of the\n",
    "model. However, in my experience, using separate token embedding and output layers\n",
    "results in better training and model performance; hence, we are using separate layers in\n",
    "our GPTModel implementation. The same is true for modern LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Lastly, let us compute the memory requirements of the 163 million parameters in our\n",
    "GPTModel object:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In conclusion, by calculating the memory requirements for the 163 million parameters in\n",
    "our GPTModel object and assuming each parameter is a 32-bit float taking up 4 bytes, we\n",
    "find that the total size of the model amounts to 621.83 MB, illustrating the relatively large\n",
    "storage capacity required to accommodate even relatively small LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In this section, we implemented the GPTModel architecture and saw that it outputs\n",
    "numeric tensors of shape [batch_size, num_tokens, vocab_size]. In the next section,\n",
    "we will write the code to convert these output tensors into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 7: GENERATING TEXT FROM OUTPUT TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let us implement the token-generation process as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: idx is a (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "Step 2: Crop current context if it exceeds the supported context size E.g., if LLM supports only 5 tokens, and the\n",
    "context size is 10 then only the last 5 tokens are used as context\n",
    "\n",
    "Step 3: Focus only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "\n",
    "Step 4: probas has shape (batch, vocab_size)\n",
    "\n",
    "Step 5: idx_next has shape (batch, 1)\n",
    "\n",
    "Step 6: Append sampled index to the running sequence, where idx has shape (batch, n_tokens+1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    ###Input batch:\n",
    " ###tensor([[6109, 3626, 6100,  345],\n",
    "        ##[6109, 1110, 6622,  257]])\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In the preceeding code, the generate_text_simple function, we use a softmax function to\n",
    "convert the logits into a probability distribution from which we identify the position with the\n",
    "highest value via torch.argmax. \n",
    "\n",
    "The softmax function is monotonic, meaning it preserves\n",
    "the order of its inputs when transformed into outputs. \n",
    "\n",
    "So, in practice, the softmax step is\n",
    "redundant since the position with the highest score in the softmax output tensor is the\n",
    "same position in the logit tensor. \n",
    "\n",
    "In other words, we could apply the torch.argmax function\n",
    "to the logits tensor directly and get identical results. \n",
    "\n",
    "However, we coded the conversion to\n",
    "illustrate the full process of transforming logits to probabilities, which can add additional\n",
    "intuition, such as that the model generates the most likely next token, which is known as\n",
    "greedy decoding.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In the next chapter, when we will implement the GPT training code, we will also\n",
    "introduce additional sampling techniques where we modify the softmax outputs such that\n",
    "the model doesn't always select the most likely token, which introduces variability and\n",
    "creativity in the generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now try out the generate_text_simple function with the \"Hello, I am\" context\n",
    "as model input\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "First, we encode the input context into token IDs:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we put the model into .eval() mode, which disables random components like\n",
    "dropout, which are only used during training, and use the generate_text_simple function\n",
    "on the encoded input tensor:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "We disable dropout since we are not training the model\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #A\n",
    "#model = GPTModel(GPT_CONFIG_124M)\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the .decode method of the tokenizer, we can convert the IDs back into text:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, based on the preceding output, the model generated gibberish, which is not\n",
    "at all coherent text. \n",
    "\n",
    "What happened? \n",
    "\n",
    "The reason why the model is unable to produce coherent text is that we haven't trained it yet. \n",
    "\n",
    "So far, we just\n",
    "implemented the GPT architecture and initialized a GPT model instance with initial random\n",
    "weights.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING GENERATIVE TEXT MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Model class we coded earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPT to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We initialize a GPT model using the code from the previous chapter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "We reduce the context length (context_length) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 tokens\n",
    "\n",
    "This is so that more readers will be able to follow and execute the code examples on their laptop computer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Next, we use the generate_text_simple function from the previous chapter to generate text.\n",
    "\n",
    "In addition, we define two convenience functions, text_to_token_ids and token_ids_to_text, for converting between token and text representations that we use throughout this chapter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see above, the model does not produce good text because it has not been trained yet\n",
    "\n",
    "How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?\n",
    "\n",
    "The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress\n",
    "\n",
    "The next chapters on finetuning LLMs will also introduce additional ways to measure model quality\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Suppose we have an inputs tensor containing the token IDs for 2 training examples (rows)\n",
    "\n",
    "Corresponding to the inputs, the targets contain the desired token IDs that we want the model to generate\n",
    "\n",
    "Notice that the targets are the inputs shifted by 1 position, as explained in chapter 2 when we implemented the data loader\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Feeding the inputs to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each\n",
    "\n",
    "Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary\n",
    "\n",
    "Applying the softmax function, we can turn the logits tensor into a tensor of the same dimension containing probability scores\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "As discussed in the previous chapter, we can apply the argmax function to convert the probability scores into predicted token IDs.\n",
    "\n",
    "The softmax function above produced a 50,257-dimensional vector for each token; the argmax function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token.\n",
    "\n",
    "Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs:                                                                                                                                                                                         \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens.\n",
    "\n",
    "That's because the model wasn't trained yet.\n",
    "\n",
    "To train the model, we need to know how far it is away from the correct predictions (targets)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The token probabilities corresponding to the target indices are as follows:\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([0.0001, 0.0000, 0.0000])\n",
      "Text 2: tensor([0.0000, 0.0001, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We want to maximize all these values, bringing them close to a probability of 1.\n",
    "    \n",
    "In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we compute the average log probability:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The goal is to make this average log probability as large as possible by optimizing the model weights.\n",
    "\n",
    "Due to the log, the largest possible value is 0, and we are currently far away from 0.\n",
    "\n",
    "In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0.\n",
    "\n",
    "The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "PyTorch already implements a cross_entropy function that carries out the previous steps\n",
    "\n",
    "Before we apply the cross_entropy function, let's check the shape of the logits and targets\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For the cross_entropy function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that the targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize.\n",
    "    \n",
    "The cross_entropy function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "A concept related to the cross-entropy loss is the perplexity of an LLM.\n",
    "\n",
    "The perplexity is simply the exponential of the cross-entropy loss.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens).\n",
    "\n",
    "In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset.\n",
    "    \n",
    "Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We use a relatively small dataset for training the LLM (in fact, only one short story)\n",
    "\n",
    "The reasons are:\n",
    "\n",
    "You can run the code examples in a few minutes on a laptop computer without a suitable GPU.\n",
    "\n",
    "The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes.\n",
    "    \n",
    "We use a text from the public domain, which can be included in this GitHub repository without violating any usage rights or bloating the repository size.\n",
    "    \n",
    "For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens\n",
    "\n",
    "At the time of this writing, the hourly cost of an 8xA100 cloud server at AWS is approximately 30 dollars. \n",
    "\n",
    "So, via an off-the-envelope calculation, training this LLM would cost 184,320 / 8 * 30 = 690,000 dollars\n",
    "\n",
    "Below, we use the same dataset we used in chapter 2.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "A quick check that the text loaded ok by printing the first and last 100 words\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE VERDICT\n",
      "June 1908\n",
      "\n",
      "I had always thought Jack Gisburn rather a cheap genius--though a\n",
      "\n",
      "good fell\n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and alone, and happen once--but\n",
      "\n",
      "there's no exterminating our kind of art.\"\n",
      "\n",
      "The End of The Verdict\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20781\n",
      "Tokens: 5774\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "With 5,145 tokens, the text is very short for training an LLM, but again, it's for educational purposes (we will also load pretrained weights later).\n",
    "\n",
    "Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training.\n",
    "    \n",
    "Since we train the LLM to predict the next word in the text, the targets look the same as these inputs, except that the targets are shifted by one position    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the DataLoader:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "We use a relatively small batch size to reduce the computational resource demand, and because the dataset is very small to begin with.\n",
    "\n",
    "Llama 2 7B was trained with a batch size of 1024, for example.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "An optional check that the data was loaded correctly:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "An optional check that the data was loaded correctly:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 5120\n",
      "Validation tokens: 512\n",
      "All tokens: 5632\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the GPT Model class we coded earlier. We will need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we implement a utility function to calculate the cross-entropy loss of a given batch.\n",
    "\n",
    "In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code.\n",
    "    \n",
    "Via the device setting, we ensure that the data is loaded onto the same device as the LLM model.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.977050971984863\n",
      "Validation loss: 11.003796577453613\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP FOR THE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Initialize lists to track losses and tokens seen\n",
    "\n",
    "Step 2: Start the main training loop\n",
    "\n",
    "Step 3: Reset loss gradients from previous batch iteration\n",
    "\n",
    "Step 4: Calculate loss gradients\n",
    "\n",
    "Step 5: Update model weights using loss gradients\n",
    "\n",
    "Step 6: Optional evaluation step\n",
    "\n",
    "Step 7: Print a sample text after each epoch\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The evaluate_model function calculates the loss over the training and\n",
    "validation set while ensuring the model is in evaluation mode with gradient tracking and\n",
    "dropout disabled when calculating the loss over the training and validation sets\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The generate_and_print_sample function is a convenience function that we use to track whether the model improves during the training. \n",
    "\n",
    "In particular, the generate_and_print_sample function takes a text snippet (start_context) as input,\n",
    "converts it into token IDs, and feeds it to the LLM to generate a text sample using the\n",
    "generate_text_simple function we used earlier\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's see this all in action by training a GPTModel instance for 10 epochs using an AdamW\n",
    "optimizer and the train_model_simple function we defined earlier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.408, Val loss 9.478\n",
      "Ep 1 (Step 000005): Train loss 7.784, Val loss 7.937\n",
      "Every effort moves you                                                  \n",
      "Ep 2 (Step 000010): Train loss 6.500, Val loss 6.818\n",
      "Ep 2 (Step 000015): Train loss 5.832, Val loss 6.396\n",
      "Every effort moves you, and                                                \n",
      "Ep 3 (Step 000020): Train loss 5.723, Val loss 6.331\n",
      "Ep 3 (Step 000025): Train loss 5.490, Val loss 6.383\n",
      "Every effort moves you.                                                 \n",
      "Ep 4 (Step 000030): Train loss 5.418, Val loss 6.362\n",
      "Ep 4 (Step 000035): Train loss 5.313, Val loss 6.396\n",
      "Every effort moves you.                                                 \n",
      "Ep 5 (Step 000040): Train loss 5.028, Val loss 6.366\n",
      "Ep 5 (Step 000045): Train loss 5.070, Val loss 6.455\n",
      "Every effort moves you.                                                 \n",
      "Ep 6 (Step 000050): Train loss 4.731, Val loss 6.281\n",
      "Ep 6 (Step 000055): Train loss 4.331, Val loss 6.185\n",
      "Every effort moves you was      \"I was his a little to the picture was not to see                              \n",
      "Ep 7 (Step 000060): Train loss 4.176, Val loss 6.179\n",
      "Ep 7 (Step 000065): Train loss 3.596, Val loss 6.061\n",
      "Every effort moves you know      \"I had to me, and to have the                        \"I was a little, and he had\n",
      "Ep 8 (Step 000070): Train loss 3.138, Val loss 6.107\n",
      "Ep 8 (Step 000075): Train loss 2.761, Val loss 6.040\n",
      "Every effort moves you know.     \"Oh, my dear Rickham! I was a little: \"Yes, I had                \"Oh, I had always to my eyes\n",
      "Ep 9 (Step 000080): Train loss 2.511, Val loss 6.038\n",
      "Ep 9 (Step 000085): Train loss 2.149, Val loss 6.068\n",
      "Every effort moves you know; and I   \"Oh, my dear Rickham! I was posing to myself like one             \"Oh, and left alone with him. I had always to  \n",
      "Ep 10 (Step 000090): Train loss 1.920, Val loss 6.089\n",
      "Ep 10 (Step 000095): Train loss 1.471, Val loss 6.133\n",
      "Every effort moves you know.   I was princely, my dear Rickham! I was posing to myself like one     \"Oh.    \"I turned, and left alone with him. I had sent all my \n",
      "Training completed in 9.73 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, based on the results printed during the training, the training loss improves\n",
    "drastically, starting with a value of 9.781 and converging to 0.391. \n",
    "\n",
    "The language skills of\n",
    "the model have improved quite a lot. In the beginning, the model is only able to append\n",
    "commas to the start context (\"Every effort moves you,,,,,,,,,,,,\") or repeat the\n",
    "word \"and\". \n",
    "\n",
    "At the end of the training, it can generate grammatically correct text.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Similar to the training set loss, we can see that the validation loss starts high (9.856)\n",
    "and decreases during the training. \n",
    "\n",
    "However, it never becomes as small as the training set\n",
    "loss and remains at 6.372 after the 10th epoch.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's create a simple plot that shows the training and validation set losses side by side\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Both the training and validation losses start to improve for the first\n",
    "epoch. However, the losses start to diverge past the second epoch. \n",
    "\n",
    "This divergence and the\n",
    "fact that the validation loss is much larger than the training loss indicate that the model is\n",
    "overfitting to the training data. \n",
    "\n",
    "We can confirm that the model memorizes the training data\n",
    "verbatim by searching for the generated text snippets, such as \"quite insensible to the\n",
    "irony\" in the \"The Verdict\" text file.\n",
    "\n",
    "\n",
    "This memorization is expected since we are working with a very, very small training\n",
    "dataset and training the model for multiple epochs. \n",
    "\n",
    "Usually, it's common to train a model\n",
    "on a much, much larger dataset for only one epoch.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODING STRATEGIES TO CONTROL RANDOMNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, we briefly revisit the generate_text_simple function\n",
    "from the previous chapter that we used inside the generate_and_print_sample earlier in\n",
    "this chapter. \n",
    "\n",
    "Then, we will cover two techniques, temperature scaling, and top-k sampling,\n",
    "to improve this function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We begin by transferring the model back from the GPU to the CPU since inference with a\n",
    "relatively small model does not require a GPU. Also, after training, we put the model into\n",
    "evaluation model to turn off random components such as dropout:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we plug the GPTModel instance (model) into the generate_text_simple function,\n",
    "which uses the LLM to generate one token at a time:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know.\n",
      "\n",
      "\n",
      "I was princely, my dear Rickham! I was posing to myself like one\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODING STRATEGY 1: TEMPERATURE SCALING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Previously, inside the generate_text_simple function, we always sampled the token\n",
    "with the highest probability as the next token using torch.argmax, also known as greedy\n",
    "decoding. \n",
    "\n",
    "To generate text with more variety, we can replace the argmax with a function\n",
    "that samples from a probability distribution (here, the probability scores the LLM generates\n",
    "for each vocabulary entry at each token generation step).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To illustrate the probabilistic sampling with a concrete example, let's briefly discuss the\n",
    "next-token generation process using a very small vocabulary for illustration purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, assume the LLM is given the start context \"every effort moves you\" and\n",
    "generates the following next-token logits:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "next_token_logits2 = next_token_logits/0.1\n",
    "\n",
    "next_token_logits3 = next_token_logits/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As discussed in the previous chapter, inside the generate_text_simple, we convert the\n",
    "logits into probabilities via the softmax function and obtain the token ID corresponding the\n",
    "generated token via the argmax function, which we can then map back into text via the\n",
    "inverse vocabulary:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.9910, 0.0000, 0.0000, 0.0000, 0.0090, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits2, dim=0)\n",
    "\n",
    "print(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1546, 0.0750, 0.0429, 0.2421, 0.0869, 0.0454, 0.0430, 0.2203, 0.0898])\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits3, dim=0)\n",
    "\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0609, 0.0016, 0.0001, 0.5721, 0.0034, 0.0001, 0.0001, 0.3576, 0.0040])\n",
      "3\n",
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "print(probas)\n",
    "\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "print(next_token_id)\n",
    "\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To implement a probabilistic sampling process, we can now replace the argmax with the\n",
    "multinomial function in PyTorch:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The printed output is \"forward\" just like before. What happened? The multinomial\n",
    "function samples the next token proportional to its probability score. \n",
    "\n",
    "In other words,\n",
    "\"forward\" is still the most likely token and will be selected by multinomial most of the\n",
    "time but not all the time. \n",
    "\n",
    "To illustrate this, let's implement a function that repeats this\n",
    "sampling 1000 times:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see based on the output, the word \"forward\" is sampled most of the time (582\n",
    "out of 1000 times), but other tokens such as \"closer\", \"inches\", and \"toward\" will also\n",
    "be sampled some of the time. \n",
    "\n",
    "This means that if we replaced the argmax function with the\n",
    "multinomial function inside the generate_and_print_sample function, the LLM would\n",
    "sometimes generate texts such as \"every effort moves you toward\", \"every effort\n",
    "moves you inches\", and \"every effort moves you closer\" instead of \"every effort\n",
    "moves you forward\".\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can further control the distribution and selection process via a concept called\n",
    "temperature scaling, where temperature scaling is just a fancy description for dividing the\n",
    "logits by a number greater than 0:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Temperatures greater than 1 result in more uniformly distributed token probabilities,\n",
    "and Temperatures smaller than 1 will result in more confident (sharper or more peaky)\n",
    "distributions.\n",
    "\n",
    "Let's illustrate this by plotting the original probabilities alongside\n",
    "probabilities scaled with different temperature values:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "##Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "A temperature of 1 divides the logits by 1 before passing them to the softmax function to\n",
    "compute the probability scores. \n",
    "\n",
    "In other words, using a temperature of 1 is the same as not\n",
    "using any temperature scaling. \n",
    "\n",
    "In this case, the tokens are selected with a probability equal\n",
    "to the original softmax probability scores via the multinomial sampling function in PyTorch.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Applying very small temperatures, such as 0.1, will\n",
    "result in sharper distributions such that the behavior of the multinomial function selects\n",
    "the most likely token (here: \"forward\") almost 100% of the time, approaching the\n",
    "behavior of the argmax function. \n",
    "\n",
    "Vice versa, a temperature of 5 results in a more uniform\n",
    "distribution where other tokens are selected more often. \n",
    "\n",
    "This can add more variety to the\n",
    "generated texts but also more often results in nonsensical text. \n",
    "\n",
    "For example, using the\n",
    "temperature of 5 results in texts such as \"every effort moves you pizza\" about 4% of\n",
    "the time.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODING STRATEGY 2: Top-k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous section, we implemented a probabilistic sampling approach coupled with\n",
    "temperature scaling to increase the diversity of the outputs. \n",
    "\n",
    "We saw that higher\n",
    "temperature values result in more uniformly distributed next-token probabilities, which\n",
    "result in more diverse outputs as it reduces the likelihood of the model repeatedly selecting\n",
    "the most probable token. \n",
    "\n",
    "This method allows for exploring less likely but potentially more\n",
    "interesting and creative paths in the generation process. \n",
    "\n",
    "However, One downside of this\n",
    "approach is that it sometimes leads to grammatically incorrect or completely nonsensical\n",
    "outputs such as \"every effort moves you pizza\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we introduce another concept called top-k sampling, which, when\n",
    "combined with probabilistic sampling and temperature scaling, can improve the text\n",
    "generation results.\n",
    "\n",
    "In top-k sampling, we can restrict the sampled tokens to the top-k most likely tokens\n",
    "and exclude all other tokens from the selection process by masking their probability scores.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Subsequently, we apply PyTorch's where function to set the logit values of tokens that are\n",
    "below the lowest logit value within our top-3 selection to negative infinity (-inf).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float(\"-inf\")), \n",
    "    other=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Lastly, let's apply the softmax function to turn these into next-token probabilities:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Temperature Scaling and Top-k sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can now apply the temperature scaling and multinomial function for probabilistic\n",
    "sampling introduced in the previous section to select the next token among these 3 nonzero probability scores to generate the next token. We do this in the next section by\n",
    "modifying the text generation function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The previous two subsections introduced two concepts to increase the diversity of LLMgenerated text: temperature sampling and top-k sampling. In this section, we combine and\n",
    "add these concepts to modify the generate_simple function we used to generate text via\n",
    "the LLM earlier, creating a new generate function:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: For-loop is the same as before: Get logits, and only focus on last time step\n",
    "\n",
    "Step 2: In this new section, we filter logits with top_k sampling\n",
    "\n",
    "Step 3: This is the new section where we apply temperature scaling\n",
    "    \n",
    "Step 4: Carry out greedy next-token selection as before when temperature scaling is disabled\n",
    "\n",
    "Step 5: Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now see this new generate function in action:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves youEveryiliaralso stabbed OrleansAllowsean 52anche crime winter unbeaten quoteembedreportprint earning\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we can see, the generated text is very different from the one we previously generated\n",
    "via the generate_simple function earlier (\"Every effort moves\n",
    "you know,\" was one of the axioms he laid...!\"), which was a memorized passage\n",
    "from the training set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING AND SAVING MODEL WEIGHTS IN PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Fortunately, saving a PyTorch model is relatively straightforward. \n",
    "\n",
    "The recommended way is to save a model's so-called state_dict, a dictionary mapping each layer to its parameters,\n",
    "using the torch.save function as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In the preceding code, \"model.pth\" is the filename where the state_dict is saved. \n",
    "\n",
    "The .pth extension is a convention for PyTorch files, though we could technically use any file\n",
    "extension.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Then, after saving the model weights via the state_dict, we can load the model\n",
    "weights into a new GPTModel model instance as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "If we plan to continue pretraining a model later, for example, using the\n",
    "train_model_simple function we defined earlier in this chapter, saving the optimizer state\n",
    "is also recommended.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Adaptive optimizers such as AdamW store additional parameters for each model weight.\n",
    "AdamW uses historical data to adjust learning rates for each model parameter dynamically.\n",
    "                                                   \n",
    "Without it, the optimizer resets, and the model may learn suboptimally or even fail to\n",
    "converge properly, which means that it will lose the ability to generate coherent text. \n",
    "\n",
    "Using\n",
    "torch.save, we can save both the model and optimizer state_dict contents as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Then, we can restore the model and optimizer states as follows by first loading the saved\n",
    "data via torch.load and then using the load_state_dict method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING PRETRAINED WEIGHTS FROM OPENAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Previously, for educational purposes, we trained a small GPT-2 model using a limited\n",
    "dataset comprising a short-story book.\n",
    "\n",
    "This approach allowed us to focus on the\n",
    "fundamentals without the need for extensive time and computational resources.\n",
    "\n",
    "    \n",
    "Fortunately, OpenAI openly shared the weights of their GPT-2 models, thus eliminating\n",
    "the need to invest tens to hundreds of thousands of dollars in retraining the model on a\n",
    "large corpus ourselves.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "In the remainder of this section, we load these weights into our GPTModel class and use\n",
    "the model for text generation. \n",
    "\n",
    "Here, weights refer to the weight parameters that are stored\n",
    "in the .weight attributes of PyTorch's Linear and Embedding layers, for example. \n",
    "\n",
    "We accessed them earlier via model.parameters() when training the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Note that OpenAI originally saved the GPT-2 weights via TensorFlow, which we have to\n",
    "install to load the weights in Python. \n",
    "\n",
    "Moreover, the following code will use a progress bar\n",
    "tool called tqdm to track the download process, which we also have to install.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 2.15.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow>=2.15.0 tqdm>=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "tqdm version: 4.66.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tqdm version:\", tqdm.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We download the gpt_download.py Python module directly from this chapter's online repository\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We can now import the download_and_load_gpt2 function from the gpt_download.py\n",
    "file as follows, which will load the GPT-2 architecture settings (settings) and weight\n",
    "parameters (params) into our Python session:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download3 import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "hparams.json: 100%|| 90.0/90.0 [00:00<00:00, 52.5kiB/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "After the execution of the previous code has been completed, let's inspect the contents of\n",
    "settings and params:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Both settings and params are Python dictionaries. The settings dictionary stores the LLM\n",
    "architecture settings similarly to our manually defined GPT_CONFIG_124M settings. \n",
    "\n",
    "The\n",
    "params dictionary contains the actual weight tensors. \n",
    "\n",
    "    \n",
    "Note that we only printed the\n",
    "dictionary keys because printing the weight contents would take up too much screen space\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We can inspect these weight tensors by printing the whole dictionary via\n",
    "print(params) or by selecting individual tensors via the respective dictionary keys, for\n",
    "example, the embedding layer weights:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We downloaded and loaded the weights of the smallest GPT-2 model via the\n",
    "download_and_load_gpt2(model_size=\"124M\", ...) setting. However, note that OpenAI\n",
    "also shares the weights of larger models: \"355M\", \"774M\", and \"1558M\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Above, we loaded the 124M GPT-2 model weights into Python, however we still need to transfer them into our GPTModel instance.\n",
    "\n",
    "First, we initialize a new GPTModel instance.\n",
    "\n",
    "Note that the original GPT model initialized the linear layers for the query, key, and value matrices in the multi-head attention module with bias vectors, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting qkv_bias to True in our implementation, too.\n",
    "                                                                                                                                                                                                                                                                                                                                  \n",
    "We are also using the 1024 token context length that was used by the original GPT-2 model(s)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Careful readers may remember that we used a 256-token length earlier, but the original\n",
    "GPT-2 models from OpenAI were trained with a 1,024-token length, so we have to update\n",
    "the NEW_CONFIG accordingly:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Also, OpenAI used bias vectors in the multi-head attention module's linear layers to\n",
    "implement the query, key, and value matrix computations. \n",
    "\n",
    "Bias vectors are not commonly\n",
    "used in LLMs anymore as they don't improve the modeling performance and are thus\n",
    "unnecessary. \n",
    "\n",
    "However, since we are working with pretrained weights, we need to match the\n",
    "settings for consistency and enable these bias vectors:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "By default, the GPTModel instance is initialized with random weights for pretraining. \n",
    "\n",
    "The last\n",
    "step to using OpenAI's model weights is to override these random weights with the weights\n",
    "we loaded into the params dictionary.\n",
    "\n",
    "For this, we will first define a small assign utility function that checks whether two\n",
    "tensors or arrays (left and right) have the same dimensions or shape and returns the\n",
    "right tensor as trainable PyTorch parameters:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Next, we define a load_weights_into_gpt function that loads the weights from the params\n",
    "dictionary into a GPTModel instance gpt:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Setting the model's positional and token embedding weights to those specified in params.\n",
    "\n",
    "Step 2: Iterate over each transformer block in the model.\n",
    "\n",
    "Step 3: The np.split function is used to divide the attention and bias weights into three equal parts for the query,\n",
    "key, and value components.\n",
    "    \n",
    "Step 4: The original GPT-2 model by OpenAI reused the token embedding weights in the output layer to reduce the\n",
    "total number of parameters, which is a concept known as weight tying.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the load_weights_into_gpt function, we carefully match the weights from OpenAI's\n",
    "implementation with our GPTModel implementation. \n",
    "\n",
    "To pick a specific example, OpenAI\n",
    "stored the weight tensor for the output projection layer for the first transformer block as\n",
    "params[\"blocks\"][0][\"attn\"][\"c_proj\"][\"w\"]. \n",
    "                                                        \n",
    "In our implementation, this weight\n",
    "tensor corresponds to gpt.trf_blocks[b].att.out_proj.weight, where gpt is a\n",
    "GPTModel instance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Developing the load_weights_into_gpt function took a lot of guesswork since OpenAI\n",
    "used a slightly different naming convention from ours. \n",
    "\n",
    "However, the assign function would\n",
    "alert us if we try to match two tensors with different dimensions. \n",
    "\n",
    "Also, if we made a\n",
    "mistake in this function, we would notice this as the resulting GPT model would be unable\n",
    "to produce coherent text.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now try the load_weights_into_gpt out in practice and load the OpenAI model\n",
    "weights into our GPTModel instance gpt:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "If the model is loaded correctly, we can now use it to generate new text using our previous\n",
    "generate function:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on this side of the river?\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can be confident that we loaded the model weights correctly because the model can\n",
    "produce coherent text. \n",
    "\n",
    "A tiny mistake in this process would cause the model to fail.\n",
    "\n",
    "    \n",
    "In the following chapters, we will work further with this pretrained model and fine-tune it\n",
    "to classify text and follow instructions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNING FOR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNLOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # Create an unverified SSL context\n",
    "    ssl_context = ssl._create_unverified_context()\n",
    "\n",
    "    # Downloading the file\n",
    "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # Unzipping the file\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    # Add .tsv file extension\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "After executing the preceding code, the dataset is saved as a tab-separated text file,\n",
    "SMSSpamCollection.tsv, in the sms_spam_collection folder. \n",
    "\n",
    "We can load it into a pandas\n",
    "DataFrame as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will  b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will  b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "When we check the class distribution, we see that the data contains \"ham\" (i.e., \"not spam\") much more frequently than \"spam\"\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "For simplicity, and because we prefer a small dataset for educational purposes anyway (it will make it possible to finetune the LLM faster), we subsample (undersample) the dataset so that it contains 747 instances from each class\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    \n",
    "    # Count the instances of \"spam\"\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # Combine ham \"subset\" with \"spam\"\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "After executing the previous code to balance the dataset, we can see that we now have\n",
    "equal amounts of spam and non-spam messages:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we convert the \"string\" class labels \"ham\" and \"spam\" into integer class labels 0 and\n",
    "1, respectively:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This process is similar to converting text into token IDs. \n",
    "\n",
    "However, instead of using the GPT\n",
    "vocabulary, which consists of more than 50,000 words, we are dealing with just two token\n",
    "IDs: 0 and 1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We create a random_split function to split the dataset into three parts: 70% for\n",
    "training, 10% for validation, and 20% for testing. \n",
    "\n",
    "(These ratios are common in machine\n",
    "learning to train, adjust, and evaluate models.)    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    # Shuffle the entire DataFrame\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # Split the DataFrame\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# Test size is implied to be 0.2 as the remainder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n",
      "149\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(validation_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Additionally, we save the dataset as CSV (comma-separated value) files, which we can\n",
    "reuse later:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING DATALOADERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Previously, we utilized a sliding window technique to generate uniformly\n",
    "sized text chunks, which were then grouped into batches for more efficient model training.\n",
    "Each chunk functioned as an individual training instance\n",
    "\n",
    "In the case of email spam classification, have two primary options:\n",
    "\n",
    "(1) Truncate all messages to the length of the shortest message in the\n",
    "dataset or batch.\n",
    "\n",
    "(2) Pad all messages to the length of the longest message in the dataset or\n",
    "batch.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Option 1 is computationally cheaper, but it may result in significant information loss if\n",
    "shorter messages are much smaller than the average or longest messages, potentially\n",
    "reducing model performance. \n",
    "\n",
    "So, we opt for the second option, which preserves the entire\n",
    "content of all messages.\n",
    "\n",
    "To implement option 2, where all messages are padded to the length of the longest\n",
    "message in the dataset, we add padding tokens to all shorter messages. \n",
    "\n",
    "For this purpose,\n",
    "we use \"<|endoftext|>\" as a padding token, as discussed in chapter 2.\n",
    "\n",
    "    \n",
    "However, instead of appending the string \"<|endoftext|>\" to each of the text messages\n",
    "directly, we can add the token ID corresponding to \"<|endoftext|>\" to the encoded text\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we have seen earlier, we first need to implement a PyTorch Dataset, which\n",
    "specifies how the data is loaded and processed, before we can instantiate the data loaders.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For this purpose, we define the SpamDataset class.\n",
    "\n",
    "This SpamDataset class handles several key tasks: it identifies the\n",
    "longest sequence in the training dataset, encodes the text messages, and ensures that all\n",
    "other sequences are padded with a padding token to match the length of the longest\n",
    "sequence.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            # Truncate sequences if they are longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # Pad sequences to the longest sequence\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Pre-tokenize texts\n",
    "    \n",
    "Step 2: Truncate sequences if they are longer than max_length\n",
    "    \n",
    "Step 3: Pad sequences to the longest sequence\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The SpamDataset class loads data from the CSV files we created earlier, tokenizes the text\n",
    "using the GPT-2 tokenizer from tiktoken and allows us to pad or truncate the sequences to\n",
    "a uniform length determined by either the longest sequence or a predefined maximum\n",
    "length.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "This ensures each input tensor is of the same size, which is necessary to create the\n",
    "batches in the training data loader we implement next:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The code outputs 120, showing that the longest sequence contains no more than 120\n",
    "tokens, a common length for text messages. \n",
    "                       \n",
    "It's worth noting that the model can handle\n",
    "sequences of up to 1,024 tokens, given its context length limit. \n",
    "\n",
    "If your dataset includes\n",
    "longer texts, you can pass max_length=1024 when creating the training dataset in the\n",
    "preceding code to ensure that the data does not exceed the model's supported input\n",
    "(context) length.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we pad the validation and test sets to match the length of the longest training\n",
    "sequence. \n",
    "\n",
    "It's important to note that any validation and test set samples exceeding the\n",
    "length of the longest training example are truncated using\n",
    "encoded_text[:self.max_length] in the SpamDataset code we defined earlier. \n",
    "\n",
    "This\n",
    "truncation is optional; you could also set max_length=None for both validation and test\n",
    "sets, provided there are no sequences exceeding 1,024 tokens in these sets\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(test_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Using the datasets as inputs, we can instantiate the data loaders similarly to what we did earlier. \n",
    "\n",
    "However, in this case, the targets represent class labels rather than the next\n",
    "tokens in the text. \n",
    "\n",
    "For instance, choosing a batch size of 8, each batch will consist of 8\n",
    "training examples of length 120 and the corresponding class label of each example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To ensure that the data loaders are working and are indeed returning batches of the\n",
    "expected size, we iterate over the training loader and then print the tensor dimensions of\n",
    "the last batch:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "As we can see, the input batches consist of 8 training examples with 120 tokens each, as\n",
    "expected.\n",
    "\n",
    "The label tensor stores the class labels corresponding to the 8 training examples.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Lastly, to get an idea of the dataset size, let's print the total number of batches in each\n",
    "dataset:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "This concludes the data preparation. Next, we will prepare the model for\n",
    "finetuning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZING A MODEL WITH PRETRAINED WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we prepare the model we will use for the classification-finetuning to identify\n",
    "spam messages. \n",
    "\n",
    "We start with initializing the pretrained model we worked with in the\n",
    "previous chapter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we import the download_and_load_gpt function from the gpt_download3.py file we\n",
    "downloaded earlier. \n",
    "\n",
    "Furthermore, we also reuse the GPTModel class and\n",
    "load_weights_into_gpt function from chapter 5 to load the downloaded weights into the\n",
    "GPT model:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arnab\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "from gpt_download3 import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To ensure that the model was loaded correctly, let's double-check that it generates coherent text\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, before we start finetuning the model as a spam classifier, let's see if the model can\n",
    "perhaps already classify spam messages by by prompting it with instructions:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Based on the output, it's apparent that the model struggles with following instructions.\n",
    "\n",
    "This is anticipated, as it has undergone only pretraining and lacks instruction-finetuning,\n",
    "which we will explore in the upcoming chapter\n",
    "\n",
    "The next section prepares the model for classification-finetuning\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDING A CLASSIFICATION HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we modify the pretrained large language model to prepare it for\n",
    "classification-finetuning. \n",
    "\n",
    "To do this, we replace the original output layer, which maps the\n",
    "hidden representation to a vocabulary of 50,257, with a smaller output layer that maps to\n",
    "two classes: 0 (\"not spam\") and 1 (\"spam\"),\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We could technically use a single output node since we are dealing with a binary\n",
    "classification task. \n",
    "\n",
    "However, this would require modifying the loss function.\n",
    "\n",
    "Therefore, we choose a\n",
    "more general approach where the number of output nodes matches the number of\n",
    "classes. \n",
    "\n",
    "For example, for a 3-class problem, such as classifying news articles as\n",
    "\"Technology\", \"Sports\", or \"Politics\", we would use three output nodes, and so forth.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we attempt to construct the modified architecture, let's print the model\n",
    "architecture via print(model), which prints the following:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Above, we can see the GPT architecture neatly laid out. \n",
    "\n",
    "As\n",
    "discussed earlier, the GPTModel consists of embedding layers followed by 12 identical\n",
    "transformer blocks (only the last block is shown for brevity), followed by a final LayerNorm\n",
    "and the output layer, out_head.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we replace the out_head with a new output layer, as illustrated in figure 6.9, that\n",
    "we will finetune.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To get the model ready for classification-finetuning, we first freeze the model, meaning that\n",
    "we make all layers non-trainable:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Then, we replace the output layer (model.out_head), which\n",
    "originally maps the layer inputs to 50,257 dimensions (the size of the vocabulary):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that in the preceding code, we use BASE_CONFIG[\"emb_dim\"], which is equal to 768 in\n",
    "the \"gpt2-small (124M)\" model, to keep the code below more general. \n",
    "\n",
    "This means we\n",
    "can also use the same code to work with the larger GPT-2 model variants.\n",
    "\n",
    "This new model.out_head output layer has its requires_grad attribute set to True by\n",
    "default, which means that it's the only layer in the model that will be updated during\n",
    "training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "This new model.out_head output layer has its requires_grad attribute set to True by\n",
    "default, which means that it's the only layer in the model that will be updated during\n",
    "training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Additionally, we configure the last transformer block and the final LayerNorm module,\n",
    "which connects this block to the output layer, to be trainable\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Even though we added a new output layer and marked certain layers as trainable or nontrainable, we can still use this model in a similar way to previous chapters. \n",
    "\n",
    "For instance, we\n",
    "can feed it an example text identical to how we have done it in earlier chapters. For\n",
    "example, consider the following example text:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Then, we can pass the encoded token IDs to the model as usual:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In earlier chapters, a similar input would have produced an output tensor of [1, 4, 50257],\n",
    "where 50,257 represents the vocabulary size. \n",
    "\n",
    "As in previous chapters, the number of\n",
    "output rows corresponds to the number of input tokens (in this case, 4). \n",
    "\n",
    "However, each\n",
    "output's embedding dimension (the number of columns) is now reduced to 2 instead of\n",
    "50,257 since we replaced the output layer of the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Remember that we are interested in finetuning this model so that it returns a class label\n",
    "that indicates whether a model input is spam or not spam. \n",
    "\n",
    "To achieve this, we don't need to\n",
    "finetune all 4 output rows but can focus on a single output token. \n",
    "\n",
    "In particular, we will\n",
    "focus on the last row corresponding to the last output token\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To extract the last output token, illustrated in figure 6.11, from the output tensor, we\n",
    "use the following code:    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Having modified the model, the next section will detail the process of transforming the\n",
    "last token into class label predictions and calculate the model's initial prediction accuracy.\n",
    "\n",
    "Following this, we will finetune the model for the spam classification task in the subsequent\n",
    "section.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATING THE CLASSIFICATION LOSS AND ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "So far in this chapter, we have prepared the dataset, loaded a pretrained model, and\n",
    "modified it for classification-finetuning. \n",
    "           \n",
    "Before we proceed with the finetuning itself, only\n",
    "one small part remains: implementing the model evaluation functions used during\n",
    "finetuning,  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before implementing the evaluation utilities, let's briefly discuss how we convert the model\n",
    "outputs into class label predictions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the previous chapter, we computed the token ID of the next token generated by the\n",
    "LLM by converting the 50,257 outputs into probabilities via the softmax function and then\n",
    "returning the position of the highest probability via the argmax function. \n",
    "\n",
    "In this chapter, we\n",
    "take the same approach to calculate whether the model outputs a \"spam\" or \"not spam\"\n",
    "prediction for a given input, with the only difference being that we\n",
    "work with 2-dimensional instead of 50,257-dimensional outputs.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's consider the last token output from\n",
    "the previous section:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can obtain the class label via the following code:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0005, 0.9995]])\n",
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "print(probas)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "In this case, the code returns 1, meaning the model predicts that the input text is \"spam.\"\n",
    "\n",
    "Using the softmax function here is optional because the largest outputs directly correspond\n",
    "to the highest probability scores. \n",
    "\n",
    "Hence, we can simplify the\n",
    "code as follows, without using softmax:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This concept can be used to compute the so-called classification accuracy, which measures\n",
    "the percentage of correct predictions across a dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To determine the classification accuracy, we apply the argmax-based prediction code to\n",
    "all examples in the dataset and calculate the proportion of correct predictions by defining a\n",
    "calc_accuracy_loader function:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's use the function to determine the classification accuracies across various datasets\n",
    "estimated from 10 batches for efficiency:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# As of this writing, in PyTorch 2.4, the results obtained via CPU and MPS were identical.\n",
    "# However, in earlier versions of PyTorch, you may observe different results when using MPS.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#print(f\"Running on {device} device.\")\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the prediction accuracies are near a random prediction, which would be\n",
    "50% in this case. \n",
    "\n",
    "To improve the prediction accuracies, we need to finetune the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Classification accuracy is not a differentiable function, so we use cross entropy\n",
    "loss as a proxy to maximize accuracy. \n",
    "\n",
    "This is the same cross entropy loss discussed earlier. \n",
    "\n",
    "Accordingly, the calc_loss_batch function remains the same as in earlier, with one\n",
    "adjustment: we focus on optimizing only the last token, model(input_batch)[:, -1, :],\n",
    "rather than all tokens, model(input_batch):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We use the calc_loss_batch function to compute the loss for a single batch obtained from\n",
    "the previously defined data loaders. To calculate the loss for all batches in a data loader, we\n",
    "define the calc_loss_loader function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as in chapter 5\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Similar to calculating the training accuracy, we now compute the initial loss for each\n",
    "data set:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the next section, we will implement a training function to finetune the model, which\n",
    "means adjusting the model to minimize the training set loss. \n",
    "\n",
    "Minimizing the training set\n",
    "loss will help increase the classification accuracy, our overall goal\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINETUNING THE MODEL ON SUPERVISED DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we define and use the training function to finetune the pretrained LLM and\n",
    "improve its spam classification accuracy. \n",
    "    \n",
    "The training loop is the\n",
    "same overall training loop we used earlier, with the only difference being that we\n",
    "calculate the classification accuracy instead of generating a sample text for evaluating the\n",
    "model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The training function also closely mirrors\n",
    "the train_model_simple function used for pretraining the model earlier.\n",
    "                                    \n",
    "The only two distinctions are that we now track the number of training examples seen\n",
    "(examples_seen) instead of the number of tokens, and we calculate the accuracy after each\n",
    "epoch instead of printing a sample text:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Set model to training mode\n",
    "\n",
    "Step 2: Reset loss gradients from previous batch iteration\n",
    "\n",
    "Step 3: Calculate loss gradients\n",
    "\n",
    "Step 4: Update model weights using loss gradients\n",
    "\n",
    "Step 5: New: track examples instead of tokens\n",
    "\n",
    "Step 6: Optional evaluation step\n",
    "\n",
    "Step 7: Calculate accuracy after each epoch\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall the same as `train_model_simple` in chapter 5\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Calculate accuracy after each epoch\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The evaluate_model function used in the train_classifier_simple is the same as the one we used earlier.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as chapter 5\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we initialize the optimizer, set the number of training epochs, and initiate the training\n",
    "using the train_classifier_simple function. \n",
    "\n",
    "We will discuss the choice of the the number\n",
    "of training epochs after we evaluated the results. \n",
    "\n",
    "The training takes about 6 minutes on an\n",
    "M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
      "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
      "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
      "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
      "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
      "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
      "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
      "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
      "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
      "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
      "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
      "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
      "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 19.35 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We then use matplotlib to plot the loss function for the training and\n",
    "validation set:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # Create a second x-axis for examples seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see based on the sharp downward slope, the model is learning well\n",
    "from the training data, and there is little to no indication of overfitting; that is, there is no\n",
    "noticeable gap between the training and validation set losses).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Using the same plot_values function, let's now also plot the classification accuracies:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the accuracy plot in figure 6.17, the model achieves a relatively high training and\n",
    "validation accuracy after epochs 4 and 5.\n",
    "    \n",
    "However, it's important to note that we previously set eval_iter=5 when using the\n",
    "train_classifier_simple function, which means our estimations of training and\n",
    "validation performance were based on only 5 batches for efficiency during training.\n",
    "\n",
    "Now, we will calculate the performance metrics for the training, validation, and test sets\n",
    "across the entire dataset by running the following code, this time without defining the\n",
    "eval_iter value:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.21%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 95.67%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The training and test set performances are almost identical.\n",
    "\n",
    "A slight discrepancy between the training and test set accuracies suggests minimal\n",
    "overfitting of the training data. \n",
    "\n",
    "Typically, the validation set accuracy is somewhat higher\n",
    "than the test set accuracy because the model development often involves tuning\n",
    "hyperparameters to perform well on the validation set, which might not generalize as\n",
    "effectively to the test set.\n",
    "\n",
    "This situation is common, but the gap could potentially be minimized by adjusting the\n",
    "model's settings, such as increasing the dropout rate (drop_rate) or the weight_decay\n",
    "parameter in the optimizer configuration.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING THE LLM AS A SPAM CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "After finetuning and evaluating the model in the previous sections, we are now in the final\n",
    "stage of this chapter:  using the model to classify spam\n",
    "messages.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Finally, let's use the finetuned GPT-based spam classification model. \n",
    "\n",
    "The following\n",
    "classify_review function follows data preprocessing steps similar to those we used in the\n",
    "SpamDataset implemented earlier in this chapter. \n",
    "\n",
    "And then, after processing text into token\n",
    "IDs, the function uses the model to predict an integer class label, similar to what we have\n",
    "implemented earlier, and then returns the corresponding class name:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Prepare inputs to the model\n",
    "\n",
    "Step 2: Truncate sequences if they too long\n",
    "    \n",
    "Step 3: Pad sequences to the longest sequence\n",
    "\n",
    "Step 4: Add batch dimension\n",
    "\n",
    "Step 5: Model inference without gradient tracking\n",
    "    \n",
    "Step 6: Logits of the last output token\n",
    "\n",
    "Step 7: Return the classified result\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
    "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
    "\n",
    "    # Truncate sequences if they too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Return the classified result\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's try this classify_review function on an example text:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The resulting model correctly predicts \"spam\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Also, here, the model makes a correct prediction and returns a \"not spam\" label.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Finally, let's save the model in case we want to reuse the model later without having to\n",
    "train it again using the torch.save method\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Once saved, the model can be loaded as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\")\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
